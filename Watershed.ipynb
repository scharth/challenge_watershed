{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Watershed Challenge",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport re\n \nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.dates as mdates\nfrom matplotlib.lines import Line2D\n\npd.options.display.max_columns = 1100\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nsns.reset_defaults()\nsns.set(\n    rc={'figure.figsize':(7,4)}, \n    style=\"whitegrid\", \n    palette=sns.color_palette(\"hls\", 3)\n)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Overview",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We will try to predict extreme watershed events in Chile.\n\nSome of the key questions we could try to answer are:\n\n- Has the frequency of heat waves events increased over the last years?\n- Is there any relationship between heat waves and peak flow events?\n- If so, can we correlate those events with the watershed's features?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "flux_data = pd.read_csv('flux.csv')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data.date = pd.to_datetime(flux_data.date, format='%Y-%m-%d')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### General Data Quality",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let us adjust the variables names to correspond to that in the challenge documentation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "flux_data.rename(\n    columns = {\n        'basin_id':'station_code',\n        'gauge_name':'station_name'\n    },\n    inplace = True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data.describe(datetime_is_numeric=True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data.nunique()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We see that there are `372` unique values for both `station_code` and `station_name`, so we discard any observational identification issues.\n\nHowever, we see lower number of distinct values for `lat`, `lon`, `mean_elev` and `area_km2`. Given the fact that this is not a raw dataset and it's been already aggregated and enriched, we won't be inspecting this difference now due to time constraints, since it's doesn't seem relevant.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "flux_data[['station_code', 'station_name', 'lat', 'lon', 'mean_elev', 'area_km2']].drop_duplicates()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "`Rio Tolten En Teodoro Schmidt` is the only station with two distinct values for `area_km2`, but the difference is negligible. We stick to the first one in order to keep a station reference dataset with `372` rows.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "station_data = flux_data.groupby(['station_code', 'station_name', 'lat', 'lon', 'mean_elev']).agg({\n    'area_km2':max,\n    'date':'nunique'\n}).reset_index()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "station_data",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def river_name(x):\n    \n    river_name = re.search(r'(.*?)(En|Antes|A.)(.*?)$', x)\n    \n    if river_name:\n        return river_name.group(1)\n    else:\n        return x\n    \ndef place_name(x):    \n    \n    place_name = re.search(r'En (.*)', x)\n    \n    if place_name:\n        return place_name.group(1)\n    else:\n        return ''",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "station_data['river_name'] = station_data.station_name.apply(lambda x: river_name(x))\n#station_data['place_name'] = station_data.station_name.apply(lambda x: place_name(x))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "station_data",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "station_data.river_name.value_counts()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Lat, lon",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ax = sns.scatterplot(data = station_data, \n                     y = 'lat', x = 'lon',\n                     hue = 'mean_elev',)\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### One TS",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "flux_data",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def plot_one_timeserie(station_code, variable, min_date='1980-01-01', max_date='2023-01-01'):\n\n    one_timeserie = flux_data[flux_data['station_code'] == station_code].set_index('date')[variable]\n\n    sample = one_timeserie.loc[min_date:max_date]\n\n    ax = sns.lineplot(data = sample)\n    ax.set(title = f\"{flux_data[flux_data.station_code == station_code]['station_name'].tolist()[0]}\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\n\n    plt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Flux",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "plot_one_timeserie(9433001, 'flux', '1990-01-01', '2020-02-03')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Precip",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_one_timeserie(1001001, 'precip', '2000-01-01', '2010-02-03')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Temp_max",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_one_timeserie(1001002, 'temp_max', '2005-01-01', '2010-02-03')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Three TS",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def plot_three_timeseries(station_code, min_date='1980-01-01', max_date='2023-01-01'):\n\n    three_timeseries = flux_data[flux_data.station_code == station_code].set_index('date')\n\n    sample = three_timeseries.loc[min_date:max_date][['temp_max', 'precip', 'flux']]\n    \n    normalized_sample = (sample - sample.mean()) / sample.std()\n\n    ax = sns.lineplot(data = normalized_sample)\n    ax.set(title = f\"{three_timeseries['station_name'].tolist()[0]}\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\n\n    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n\n    plt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot_three_timeseries(1001001, '2000-01-01', '2010-02-03')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot_three_timeseries(1001001, '2000-01-01', '2010-02-03')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Extremes",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We proceed with a working example for station `1001002`.\n\nThis first exercise takes into account the seasonality of the phenomena and only then we look at the percentiles to catch the days that were atypical, in an univariate sense, to identify these days as extreme ones.\n\nIn the following chunks, we use `statsmodels.tsa.seasonal.seasonal_decompose` to subtract the seasonal component of each series.\nEventual missing points are interpolated using `method='linear'`, and the decomposition is applied using the additive approach, for the sake of simplicity.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "single_ts = flux_data[flux_data.station_code == 1001002]\\\n.set_index('date')\\\n.asfreq('D')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "single_ts",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "single_ts_in = flux_data[flux_data.station_code == 1001002]\\\n.set_index('date')\\\n.asfreq('D')\\\n.interpolate(method='linear')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Additive model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "result = seasonal_decompose(single_ts_in['flux'], model=\"additive\", period = 365)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt = result.plot()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "result_values = pd.DataFrame([result.observed, result.seasonal, result.trend, result.resid]).T.dropna()\nresult_values['excess'] = result_values['flux'] - result_values['seasonal']\nresult_values.describe()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "extremes = result_values[result_values.excess > result_values.excess.quantile(0.95)]\nextremes",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ax = sns.lineplot(data = result_values.excess)\nax.set(title = f\"Seasonaly adjusted flux for 1001002 station\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ax = sns.lineplot(data = result_values.flux)\nax.set(title = f\"Flux for 1001002 station\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ax = sns.lineplot(data = extremes.excess)\nax.set(title = f\"Only extremes for 1001002 station\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "extremes_index = extremes.index\nextremes_index",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Conclusions",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> This variables should take the value of 1 when that variable in a specific day was extreme. Being extreme could be considered as being greater than expected. For example, a flux can be considered as extreme (value 1) when is over the 95 percentile of the flux distribution for that specific season, and takes the value 0 otherwise. Taking into account the seasonality of that variables is very important, because  could be considered as extreme in wintertime, but itâ€™d be a normal temperature for summertime.\n\n1. Do you consider this a good way of capturing extreme events?\n\nIt's a fair way. It makes sense to account for seasonality. Since our context is global warming, we would welcome some approach that references directly the trend component, for instance. \"Greater than expected\" is subjective. If we consider that we shouldn't be expecting an underlying positive trend, looking for the deviation from the seasonal component fulfills the objective at first sight.\n\n2. Or you would have used a different method? Which one?\n\nNevertheless, we can see from the original time series plot for the example above that excess flux are easily recognizable without any advanced statistical procedures. This raises a point on which aspect of the problem concerns us the most. A flux of 3 units in the dry season is as relevant as a flux of 20 units in the rainy season, given that both values represent a 0.95 percentile for that season? Is it a substancial occurrence?\nNature has ways to deal with these uncommon events, for instance, the ground absortion of water. So given two water sprouts events of the same magnitude, the one that happens when the soil is dry is less critical than the one that happens after a week or two of daily strong water sprouts, when the soil were already soaked.\nThe same could go for heat waves, once we could be concern with direct impact on human beings, so despite of seasonality being important, we could study to ignore it and only look for time-invariant variability.\nEither ways, this invites an approach of moving averages for instance. And we could then naively look for extreme events (perc > 0.95) .",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Wrap-up",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's turn this into a function and loop through all stations and variables.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_extremes(station_code, variable):\n    \n    single_ts_in = flux_data[flux_data.station_code == station_code].set_index('date')\\\n    [variable]\\\n    .asfreq('D')\\\n    .interpolate(method='linear')\n    \n    result = seasonal_decompose(single_ts_in, model=\"additive\", period=365)\n    \n    result_values = pd.DataFrame([result.observed, result.seasonal]).T.dropna()\n    result_values['excess'] = result_values[variable] - result_values['seasonal']\n    \n    extremes = result_values[result_values.excess > result_values.excess.quantile(0.95)]\n    \n    return extremes.index",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data.loc[:, 'flux_extreme'] = 0\nflux_data.loc[:, 'temp_max_extreme'] = 0\nflux_data.loc[:, 'precip_extreme'] = 0",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for variable in ['flux', 'temp_max', 'precip']:\n    \n    print(variable)\n    \n    for station_code in station_data[station_data.date > 730].reset_index()['station_code']:\n                \n        extremes = get_extremes(station_code, variable)\n        \n        flux_data.loc[\n            (flux_data.station_code == station_code) & (flux_data.date.isin(extremes)), \n            f\"{variable}_extreme\"\n        ] = 1",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flux_data[['flux_extreme','temp_max_extreme','precip_extreme']].isna().sum()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#flux_data.to_csv('flux_data_rich.csv')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Monthly",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "monthly_mean = flux_data.resample(rule='M', on='date').mean()[['flux_extreme','temp_max_extreme','precip_extreme']]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "monthly_mean",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ax = sns.lineplot(data = monthly_mean.loc[:'2020-05-01'])\nax.set(title = f\"ueh\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Annualy",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "annual_mean = flux_data.resample(rule='Y', on='date').mean()[['flux_extreme','temp_max_extreme','precip_extreme']]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "annual_mean",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ax = sns.lineplot(data = annual_mean.loc[:'2020-05-01'])\nax.set(title = f\"ueh\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%y'))\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Questions left to discuss",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "1. Are there any different behaviours among different watersheds?\n\n2. Have they become more frequent?\n\n3. Extreme flux prediction: modeling and discussion",
      "metadata": {}
    }
  ]
}